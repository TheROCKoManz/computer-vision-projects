{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "7bb16142a0a342bea5648f5e862dd123": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fbecc3b395f4af8a6b1a84bf2202431",
       "IPY_MODEL_74447ec40dc84ece88725560faad2cdb",
       "IPY_MODEL_a96c7a79e4624c6a8fcba3e420268378"
      ],
      "layout": "IPY_MODEL_4f8d701ff69249fca458411899cc6650"
     }
    },
    "3fbecc3b395f4af8a6b1a84bf2202431": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_960bcf38331e4046b47850b76b2d7cc1",
      "placeholder": "​",
      "style": "IPY_MODEL_8b2bd97101ae4197968fd94e9ba310cb",
      "value": "100%"
     }
    },
    "74447ec40dc84ece88725560faad2cdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fccff15fda5f41519368b481588ef704",
      "max": 1298,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_219e1e06adc647088ad94840a3a7a024",
      "value": 1298
     }
    },
    "a96c7a79e4624c6a8fcba3e420268378": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b0928c8fe304b5aadc0b812038872f2",
      "placeholder": "​",
      "style": "IPY_MODEL_662e9b6cc8774aacac22d07446271754",
      "value": " 1298/1298 [04:08&lt;00:00,  5.10it/s]"
     }
    },
    "4f8d701ff69249fca458411899cc6650": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960bcf38331e4046b47850b76b2d7cc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b2bd97101ae4197968fd94e9ba310cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fccff15fda5f41519368b481588ef704": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "219e1e06adc647088ad94840a3a7a024": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b0928c8fe304b5aadc0b812038872f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "662e9b6cc8774aacac22d07446271754": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4okzdHlKMaj",
    "outputId": "37b7a00d-b58d-4a65-a816-4a75efd73be4",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:10.740407767Z",
     "start_time": "2023-08-06T13:44:10.540076730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  6 19:14:10 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   51C    P8               1W /  50W |      5MiB /  4096MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1004      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "984J4pv4K2D-",
    "outputId": "727cd2eb-bed1-49f3-9f79-41093c7c81dc",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:10.796849279Z",
     "start_time": "2023-08-06T13:44:10.741065811Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manz/Desktop/My Stuffs/Work Stuffs/Python Envs/HyperSpace/ComputerVision/CrowdAnalysis/WalkInWalkOutCounter\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gtxQLn33TBWo",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:10.797137075Z",
     "start_time": "2023-08-06T13:44:10.783954857Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Pip install method (recommended)\n",
    "\n",
    "!pip install ultralytics\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ],
   "metadata": {
    "id": "IGckxTNGLKDh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "312ecddc-3c0e-4a5a-e1f7-78956d60c10f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.143 🚀 Python-3.10.12 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 3904MiB)\n",
      "Setup complete ✅ (8 CPUs, 15.5 GB RAM, 44.0/72.3 GB disk)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd {HOME}\n",
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "%cd {HOME}/ByteTrack\n",
    "\n",
    "# workaround related to https://github.com/roboflow/notebooks/issues/80"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KdBkOflo2xY",
    "outputId": "5d5d90aa-721d-4c80-8960-d99b4d825c68",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:18.649988619Z",
     "start_time": "2023-08-06T13:44:18.229102830Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manz/Desktop/My Stuffs/Work Stuffs/Python Envs/HyperSpace/ComputerVision/CrowdAnalysis/WalkInWalkOutCounter\n",
      "fatal: destination path 'ByteTrack' already exists and is not an empty directory.\r\n",
      "/home/manz/Desktop/My Stuffs/Work Stuffs/Python Envs/HyperSpace/ComputerVision/CrowdAnalysis/WalkInWalkOutCounter/ByteTrack\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "!pip3 install -q -r requirements.txt\n",
    "!pip install -e .\n",
    "!pip install -q cython_bbox\n",
    "!pip install -q onemetric\n",
    "# workaround related to https://github.com/roboflow/notebooks/issues/112 and https://github.com/roboflow/notebooks/issues/106\n",
    "!pip install -q loguru lap thop\n",
    "!pip install numpy==1.22.4\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolox.__version__: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(f\"{HOME}/ByteTrack\")\n",
    "import yolox\n",
    "print(\"yolox.__version__:\", yolox.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:39.429039224Z",
     "start_time": "2023-08-06T13:44:39.399196590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision.__version__: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install supervision==0.2.0\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "\n",
    "import supervision\n",
    "print(\"supervision.__version__:\", supervision.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision.__version__: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install supervision==0.2.0\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "\n",
    "import supervision\n",
    "print(\"supervision.__version__:\", supervision.__version__)\n",
    "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
    "from onemetric.cv.utils.iou import box_iou_batch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BYTETrackerArgs:\n",
    "    track_thresh: float = 0.25\n",
    "    track_buffer: int = 30\n",
    "    match_thresh: float = 0.8\n",
    "    aspect_ratio_thresh: float = 3.0\n",
    "    min_box_area: float = 1.0\n",
    "    mot20: bool = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Roboflow Supervision"
   ],
   "metadata": {
    "id": "_kSHFj8uQ9qe"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d60yX_PFQ9A2",
    "outputId": "6c84325b-07b8-4302-c2a6-5c75e36215ae",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:42.987717522Z",
     "start_time": "2023-08-06T13:44:42.981605453Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from supervision.draw.color import ColorPalette\n",
    "from supervision import Point\n",
    "from supervision import VideoInfo\n",
    "from supervision import get_video_frames_generator\n",
    "from supervision import VideoSink\n",
    "from supervision.notebook.utils import show_frame_in_notebook\n",
    "from supervision import Detections, BoxAnnotator\n",
    "from supervision.detection.line_counter import LineZone, LineZoneAnnotator"
   ],
   "metadata": {
    "id": "7YDohOpMTWH5",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:43.027943666Z",
     "start_time": "2023-08-06T13:44:42.985256723Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking utils\n",
    "\n",
    "Unfortunately, we have to manually match the bounding boxes coming from our model with those created by the tracker."
   ],
   "metadata": {
    "id": "mPdB-v_hWxBy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# converts Detections into format that can be consumed by match_detections_with_tracks function\n",
    "def detections2boxes(detections: Detections) -> np.ndarray:\n",
    "    return np.hstack((\n",
    "        detections.xyxy,\n",
    "        detections.confidence[:, np.newaxis]\n",
    "    ))\n",
    "\n",
    "\n",
    "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
    "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
    "    return np.array([\n",
    "        track.tlbr\n",
    "        for track\n",
    "        in tracks\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "# matches our bounding boxes with predictions\n",
    "def match_detections_with_tracks(\n",
    "    detections: Detections,\n",
    "    tracks: List[STrack]\n",
    ") -> Detections:\n",
    "    if not np.any(detections.xyxy) or len(tracks) == 0:\n",
    "        return np.empty((0,))\n",
    "\n",
    "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
    "    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
    "    track2detection = np.argmax(iou, axis=1)\n",
    "\n",
    "    tracker_ids = [None] * len(detections)\n",
    "\n",
    "    for tracker_index, detection_index in enumerate(track2detection):\n",
    "        if iou[tracker_index, detection_index] != 0:\n",
    "            tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
    "\n",
    "    return tracker_ids"
   ],
   "metadata": {
    "id": "SE0G6LvFAXlk",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:43.028326819Z",
     "start_time": "2023-08-06T13:44:43.027812710Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load pre-trained YOLOv8 model"
   ],
   "metadata": {
    "id": "c_417m4g9XVd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# settings\n",
    "%cd {HOME}"
   ],
   "metadata": {
    "id": "m3FMq5FcUsRc",
    "ExecuteTime": {
     "end_time": "2023-08-06T13:44:43.029923293Z",
     "start_time": "2023-08-06T13:44:43.028063401Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manz/Desktop/My Stuffs/Work Stuffs/Python Envs/HyperSpace/ComputerVision/CrowdAnalysis/WalkInWalkOutCounter\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict and annotate single frame"
   ],
   "metadata": {
    "id": "6to6MgPmTnCu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import supervision as sv\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "vid = \"/home/manz/Desktop/My Stuffs/Work Stuffs/Python Envs/HyperSpace/ComputerVision/Data/Crowd_Count/Sample_Videos/JapanStreet.mp4\"\n",
    "MODEL = \"yolov8x.pt\"\n",
    "model = YOLO(MODEL)\n",
    "model.fuse()\n",
    "\n",
    "# extract video frame\n",
    "generator = sv.get_video_frames_generator(vid)\n",
    "iterator = iter(generator)\n",
    "frame = next(iterator)\n",
    "\n",
    "# detect\n",
    "results = model.predict(frame, imgsz=1280)[0]\n",
    "detections = sv.Detections.from_yolov8(results)\n",
    "detections = detections[detections.class_id==0]\n",
    "\n",
    "# annotate\n",
    "box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=2, text_scale=1, text_padding = 1)\n",
    "labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n",
    "frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "%matplotlib inline\n",
    "# sv.show_frame_in_notebook(frame, (10, 10))\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global click_count, click_points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and click_count < 2:\n",
    "        click_points.append((x, y))\n",
    "        click_count += 1\n",
    "        cv2.circle(image, (x, y), 5, (0, 255, 255), -1)  # Yellow color (BGR)\n",
    "\n",
    "# Initialize variables\n",
    "click_count = 0\n",
    "click_points = []\n",
    "\n",
    "# Load the image\n",
    "image = frame\n",
    "\n",
    "# Open a window to display the image\n",
    "cv2.namedWindow('Click Points')\n",
    "for i in range(2):\n",
    "    cv2.imshow('Click Points', image,)\n",
    "cv2.setMouseCallback('Click Points', mouse_callback)\n",
    "\n",
    "# Wait for user clicks and display the points\n",
    "while click_count < 2:\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:  # Exit loop if the 'Esc' key is pressed\n",
    "        break\n",
    "\n",
    "# Close the image window\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Extract click point values\n",
    "x1, y1 = click_points[0]\n",
    "x2, y2 = click_points[1]\n",
    "\n",
    "# Print the values\n",
    "print(\"x1:\", x1, \"y1:\", y1)\n",
    "print(\"x2:\", x2, \"y2:\", y2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "hZQsgCa0cFvH",
    "outputId": "52df1e16-2137-4091-f946-f7e8507f929b",
    "ExecuteTime": {
     "end_time": "2023-08-06T07:12:55.896478225Z",
     "start_time": "2023-08-06T07:12:48.183663680Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n",
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 4 backpacks, 6 handbags, 273.3ms\n",
      "Speed: 4.3ms preprocess, 273.3ms inference, 0.9ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: 633 y1: 577\n",
      "x2: 1752 y2: 961\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict and annotate whole video"
   ],
   "metadata": {
    "id": "3ZbGmYfiT0EV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "VideoInfo.from_video_path(vid)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3btq7JavXknU",
    "outputId": "b091bf78-0c03-4139-8935-6d032c2b1e1d",
    "ExecuteTime": {
     "end_time": "2023-08-06T06:58:52.166117832Z",
     "start_time": "2023-08-06T06:58:52.141877618Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "VideoInfo(width=1920, height=1080, fps=30, total_frames=3969)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "LINE_START = Point(x1,y1)\n",
    "LINE_END = Point(x2,y2)\n",
    "# create BYTETracker instance\n",
    "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
    "# create VideoInfo instance\n",
    "video_info = VideoInfo.from_video_path(vid)\n",
    "# create frame generator\n",
    "generator = get_video_frames_generator(vid)\n",
    "# create LineCounter instance\n",
    "line_counter = LineZone(start=LINE_START, end=LINE_END)\n",
    "# create instance of BoxAnnotator and LineCounterAnnotator\n",
    "box_annotator = BoxAnnotator(thickness=2, text_thickness=2, text_scale=1, text_padding = 1)\n",
    "line_annotator = LineZoneAnnotator(thickness=3, text_thickness=2, text_scale=3, text_padding=1)\n",
    "\n",
    "# open target video file\n",
    "# loop over video frames\n",
    "for frame in generator:\n",
    "    # model prediction on single frame and conversion to supervision Detections\n",
    "    results = model.predict(frame, imgsz=1280)[0]\n",
    "    detections = sv.Detections.from_yolov8(results)\n",
    "\n",
    "    # filtering out detections with unwanted classes\n",
    "    detections = detections[detections.class_id==0]\n",
    "\n",
    "    # tracking detections\n",
    "    tracks = byte_tracker.update(\n",
    "        output_results=detections2boxes(detections=detections),\n",
    "        img_info=frame.shape,\n",
    "        img_size=frame.shape\n",
    "    )\n",
    "    tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "\n",
    "    detections.tracker_id = np.array(tracker_id)\n",
    "\n",
    "    # filtering out detections without trackers\n",
    "    mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "    detections.filter(mask=mask, inplace=True)\n",
    "\n",
    "    # format custom labels\n",
    "    labels = [\n",
    "        f\"#{tracker_id} -- {confidence:0.2f}\"\n",
    "        for _, confidence, class_id, tracker_id\n",
    "        in detections\n",
    "    ]\n",
    "\n",
    "    # updating line counter\n",
    "    line_counter.trigger(detections=detections)\n",
    "\n",
    "    # annotate and display frame\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    line_annotator.annotate(frame=frame, line_counter=line_counter)\n",
    "    \n",
    "    original_height, original_width = image.shape[:2]\n",
    "    desired_height = int(original_height * 0.7)\n",
    "    desired_width = int(original_width * 0.7)\n",
    "    small_image = cv2.resize(frame, (desired_width, desired_height))\n",
    "\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Sample_Output', small_image)\n",
    "    # result.write(frame)\n",
    "    # if detection.__len__() == 15:\n",
    "    #     breakq\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "id": "Q9ppb7bFvWfc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7bb16142a0a342bea5648f5e862dd123",
      "3fbecc3b395f4af8a6b1a84bf2202431",
      "74447ec40dc84ece88725560faad2cdb",
      "a96c7a79e4624c6a8fcba3e420268378",
      "4f8d701ff69249fca458411899cc6650",
      "960bcf38331e4046b47850b76b2d7cc1",
      "8b2bd97101ae4197968fd94e9ba310cb",
      "fccff15fda5f41519368b481588ef704",
      "219e1e06adc647088ad94840a3a7a024",
      "3b0928c8fe304b5aadc0b812038872f2",
      "662e9b6cc8774aacac22d07446271754"
     ]
    },
    "outputId": "aa322ff2-2911-4487-bb60-b053f7705c5d",
    "ExecuteTime": {
     "end_time": "2023-08-06T07:20:05.414407395Z",
     "start_time": "2023-08-06T07:20:02.005977818Z"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 4 backpacks, 6 handbags, 276.5ms\n",
      "Speed: 21.6ms preprocess, 276.5ms inference, 0.9ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 13 persons, 1 bicycle, 3 backpacks, 5 handbags, 1 suitcase, 274.8ms\n",
      "Speed: 5.9ms preprocess, 274.8ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 6 backpacks, 4 handbags, 1 suitcase, 275.4ms\n",
      "Speed: 5.2ms preprocess, 275.4ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 13 persons, 1 bicycle, 4 backpacks, 3 handbags, 1 suitcase, 278.7ms\n",
      "Speed: 5.3ms preprocess, 278.7ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 13 persons, 1 bicycle, 5 backpacks, 4 handbags, 274.8ms\n",
      "Speed: 5.6ms preprocess, 274.8ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 4 backpacks, 4 handbags, 275.1ms\n",
      "Speed: 5.3ms preprocess, 275.1ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 4 backpacks, 3 handbags, 274.9ms\n",
      "Speed: 7.8ms preprocess, 274.9ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 14 persons, 1 bicycle, 6 backpacks, 4 handbags, 275.0ms\n",
      "Speed: 5.6ms preprocess, 275.0ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 13 persons, 1 bicycle, 3 backpacks, 3 handbags, 275.7ms\n",
      "Speed: 5.3ms preprocess, 275.7ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 15 persons, 1 bicycle, 3 backpacks, 5 handbags, 275.8ms\n",
      "Speed: 6.6ms preprocess, 275.8ms inference, 1.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 15 persons, 1 bicycle, 3 backpacks, 5 handbags, 307.5ms\n",
      "Speed: 4.9ms preprocess, 307.5ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 33\u001B[0m\n\u001B[1;32m     31\u001B[0m cv2\u001B[38;5;241m.\u001B[39mnamedWindow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mClick Points\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m---> 33\u001B[0m     \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mClick Points\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m cv2\u001B[38;5;241m.\u001B[39msetMouseCallback(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mClick Points\u001B[39m\u001B[38;5;124m'\u001B[39m, mouse_callback)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Wait for user clicks and display the points\u001B[39;00m\n",
      "\u001B[0;31merror\u001B[0m: OpenCV(4.8.0) /io/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "cam = '/dev/video0'\n",
    "vid = cv2.VideoCapture(cam)\n",
    "MODEL = \"yolov8x.pt\"\n",
    "model = YOLO(MODEL)\n",
    "model.fuse()\n",
    "ret, frame = vid.read()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# sv.show_frame_in_notebook(frame, (10, 10))\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global click_count, click_points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and click_count < 2:\n",
    "        click_points.append((x, y))\n",
    "        click_count += 1\n",
    "        cv2.circle(image, (x, y), 5, (0, 255, 255), -1)  # Yellow color (BGR)\n",
    "\n",
    "# Initialize variables\n",
    "click_count = 0\n",
    "click_points = []\n",
    "\n",
    "# Load the image\n",
    "image = frame\n",
    "\n",
    "# Open a window to display the image\n",
    "cv2.namedWindow('Click Points')\n",
    "for i in range(2):\n",
    "    cv2.imshow('Click Points', image,)\n",
    "cv2.setMouseCallback('Click Points', mouse_callback)\n",
    "\n",
    "# Wait for user clicks and display the points\n",
    "while click_count < 2:\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:  # Exit loop if the 'Esc' key is pressed\n",
    "        break\n",
    "\n",
    "# Close the image window\n",
    "cv2.destroyAllWindows()\n",
    "vid.release()\n",
    "# Extract click point values\n",
    "x1, y1 = click_points[0]\n",
    "x2, y2 = click_points[1]\n",
    "\n",
    "# Print the values\n",
    "print(\"x1:\", x1, \"y1:\", y1)\n",
    "print(\"x2:\", x2, \"y2:\", y2)\n",
    "\n",
    "LINE_START = Point(x1,y1)\n",
    "LINE_END = Point(x2,y2)\n",
    "\n",
    "# create BYTETracker instance\n",
    "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
    "\n",
    "# create LineCounter instance\n",
    "line_counter = LineZone(start=LINE_START, end=LINE_END)\n",
    "# create instance of BoxAnnotator and LineCounterAnnotator\n",
    "box_annotator = BoxAnnotator(thickness=2, text_thickness=2, text_scale=1, text_padding = 1)\n",
    "line_annotator = LineZoneAnnotator(thickness=2, text_thickness=2, text_scale=1, text_padding=1)\n",
    "\n",
    "# open target video file\n",
    "# loop over video frames\n",
    "cap = cv2.VideoCapture(cam)\n",
    "# Check if the camera is opened\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(cam)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        break\n",
    "    \n",
    "    results = model.predict(frame, imgsz=1280)[0]\n",
    "    detections = sv.Detections.from_yolov8(results)\n",
    "\n",
    "    # filtering out detections with unwanted classes\n",
    "    # detections = detections[detections.class_id==0]\n",
    "\n",
    "    # tracking detections\n",
    "    tracks = byte_tracker.update(\n",
    "        output_results=detections2boxes(detections=detections),\n",
    "        img_info=frame.shape,\n",
    "        img_size=frame.shape\n",
    "    )\n",
    "    tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "\n",
    "    detections.tracker_id = np.array(tracker_id)\n",
    "\n",
    "    # filtering out detections without trackers\n",
    "    mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "    detections.filter(mask=mask, inplace=True)\n",
    "\n",
    "    # format custom labels\n",
    "    labels = [\n",
    "        f\"#{tracker_id} -- {confidence:0.2f}\"\n",
    "        for _, confidence, class_id, tracker_id\n",
    "        in detections\n",
    "    ]\n",
    "\n",
    "    # updating line counter\n",
    "    line_counter.trigger(detections=detections)\n",
    "\n",
    "    # annotate and display frame\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    line_annotator.annotate(frame=frame, line_counter=line_counter)\n",
    "    \n",
    "    cv2.imshow('Sample_Output', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T13:45:24.197231087Z",
     "start_time": "2023-08-06T13:45:22.830286864Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = cv2.VideoCapture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not open video at /dev/Video0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msupervision\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msv\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43msv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVideoInfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_video_path\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/dev/Video0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/supervision/video.py:44\u001B[0m, in \u001B[0;36mVideoInfo.from_video_path\u001B[0;34m(cls, video_path)\u001B[0m\n\u001B[1;32m     42\u001B[0m video \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mVideoCapture(video_path)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m video\u001B[38;5;241m.\u001B[39misOpened():\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not open video at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(video\u001B[38;5;241m.\u001B[39mget(cv2\u001B[38;5;241m.\u001B[39mCAP_PROP_FRAME_WIDTH))\n\u001B[1;32m     47\u001B[0m height \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(video\u001B[38;5;241m.\u001B[39mget(cv2\u001B[38;5;241m.\u001B[39mCAP_PROP_FRAME_HEIGHT))\n",
      "\u001B[0;31mException\u001B[0m: Could not open video at /dev/Video0"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "\n",
    "sv.VideoInfo.from_video_path('/dev/Video0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T14:03:42.876845452Z",
     "start_time": "2023-08-06T14:03:42.831833765Z"
    }
   }
  }
 ]
}
